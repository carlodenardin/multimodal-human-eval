{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available Models\n",
    "\n",
    "PALIGEMMA\n",
    "\n",
    "The PALIGEMMA models are open and lightweight vision-language models (VLMs) inspired by PaLI-3 and built on open components such as the SigLIP vision model and the Gemma language model. PaliGemma takes both images and text as input and can answer questions about images with details and context, meaning it can perform deeper image analysis and provide useful information, such as encoded captions for images and short videos, object detection, and text reading embedded in images.\n",
    "\n",
    "Model Types\n",
    "\n",
    "PaliGemma PT: Pre-trained models for general-purpose use that can be fine-tuned for various tasks.\n",
    "PaliGemma FT: Research-oriented models fine-tuned on specific research datasets.\n",
    "PaliGemma Mix: Models optimized for a combination of tasks that can be used immediately for common use cases.\n",
    "Available Models\n",
    "\n",
    "PALIGEMMA (3B): Not working\n",
    "PALIGEMMA 2: Params (3B, 10B, 28B); Resolutions: (224 x 224, 448 x 448, 896 x 896)\n",
    "PALIGEMMA 2 MIX (release 19-Feb-2025): Google Blog TO BE TESTED\n",
    "LLAVA\n",
    "\n",
    "LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.\n",
    "\n",
    "Available Models\n",
    "\n",
    "LLAVA 1.6: Params (7B, 13B, 34B); Resolutions (672x672, 336x1344, 1344x336) TO BE TESTED\n",
    "OPENAI - CHATGPT\n",
    "\n",
    "o3-mini (V)\n",
    "GPT-4o (V)\n",
    "GPT-4.5 (V)\n",
    "DEEPSEEK\n",
    "\n",
    "DeepSeek-VL: DeepSeek-VL models are designed to enhance multimodal understanding capabilities. The DeepSeek-VL model is built upon the DeepSeek-LLM-1.3B-base model. The model has been trained on approximately 500 billion text tokens and 400 billion vision-language tokens. DeepSeek-VL2 introduces significant improvements in performance and efficiency. This advanced series consists of multiple variants:\n",
    "DeepSeek-VL2-Tiny (1B)\n",
    "DeepSeek-VL2-Small (2.8B)\n",
    "DeepSeek-VL2 (4.5B)\n",
    "TO BE TESTED"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
